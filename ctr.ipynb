{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Start training —— GBDT...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\ttraining's binary_logloss: 0.661469\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\ttraining's binary_logloss: 0.653509\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\ttraining's binary_logloss: 0.645715\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\ttraining's binary_logloss: 0.637941\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\ttraining's binary_logloss: 0.630533\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\ttraining's binary_logloss: 0.623256\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\ttraining's binary_logloss: 0.615913\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\ttraining's binary_logloss: 0.608718\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\ttraining's binary_logloss: 0.601775\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\ttraining's binary_logloss: 0.594955\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\ttraining's binary_logloss: 0.588412\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\ttraining's binary_logloss: 0.581998\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\ttraining's binary_logloss: 0.575539\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\ttraining's binary_logloss: 0.569354\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\ttraining's binary_logloss: 0.563238\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\ttraining's binary_logloss: 0.55723\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\ttraining's binary_logloss: 0.55127\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\ttraining's binary_logloss: 0.545342\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\ttraining's binary_logloss: 0.539601\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\ttraining's binary_logloss: 0.533882\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\ttraining's binary_logloss: 0.528111\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\ttraining's binary_logloss: 0.522611\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\ttraining's binary_logloss: 0.517206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\ttraining's binary_logloss: 0.511897\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\ttraining's binary_logloss: 0.506679\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\ttraining's binary_logloss: 0.5016\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\ttraining's binary_logloss: 0.496617\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\ttraining's binary_logloss: 0.491735\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\ttraining's binary_logloss: 0.486926\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\ttraining's binary_logloss: 0.482187\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\ttraining's binary_logloss: 0.477343\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\ttraining's binary_logloss: 0.472614\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\ttraining's binary_logloss: 0.467965\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\ttraining's binary_logloss: 0.463467\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\ttraining's binary_logloss: 0.458898\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\ttraining's binary_logloss: 0.454316\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\ttraining's binary_logloss: 0.449805\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\ttraining's binary_logloss: 0.445638\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\ttraining's binary_logloss: 0.441262\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\ttraining's binary_logloss: 0.436957\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\ttraining's binary_logloss: 0.432928\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\ttraining's binary_logloss: 0.428812\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\ttraining's binary_logloss: 0.424921\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\ttraining's binary_logloss: 0.420929\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\ttraining's binary_logloss: 0.417003\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\ttraining's binary_logloss: 0.412997\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\ttraining's binary_logloss: 0.409202\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\ttraining's binary_logloss: 0.405465\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\ttraining's binary_logloss: 0.401785\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\ttraining's binary_logloss: 0.398161\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\ttraining's binary_logloss: 0.394527\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\ttraining's binary_logloss: 0.3909\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\ttraining's binary_logloss: 0.387341\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\ttraining's binary_logloss: 0.383854\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\ttraining's binary_logloss: 0.380463\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\ttraining's binary_logloss: 0.377245\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\ttraining's binary_logloss: 0.373806\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[58]\ttraining's binary_logloss: 0.370569\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\ttraining's binary_logloss: 0.36745\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\ttraining's binary_logloss: 0.364386\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\ttraining's binary_logloss: 0.361305\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\ttraining's binary_logloss: 0.358257\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[63]\ttraining's binary_logloss: 0.355129\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\ttraining's binary_logloss: 0.352164\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\ttraining's binary_logloss: 0.349135\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\ttraining's binary_logloss: 0.345996\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[67]\ttraining's binary_logloss: 0.342893\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[68]\ttraining's binary_logloss: 0.339836\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\ttraining's binary_logloss: 0.336822\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\ttraining's binary_logloss: 0.333851\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\ttraining's binary_logloss: 0.330956\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\ttraining's binary_logloss: 0.3281\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\ttraining's binary_logloss: 0.32538\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\ttraining's binary_logloss: 0.322599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[75]\ttraining's binary_logloss: 0.31983\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\ttraining's binary_logloss: 0.317151\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\ttraining's binary_logloss: 0.314527\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\ttraining's binary_logloss: 0.311935\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\ttraining's binary_logloss: 0.309216\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\ttraining's binary_logloss: 0.306619\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\ttraining's binary_logloss: 0.303924\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\ttraining's binary_logloss: 0.301277\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\ttraining's binary_logloss: 0.298661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[84]\ttraining's binary_logloss: 0.296098\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[85]\ttraining's binary_logloss: 0.293556\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\ttraining's binary_logloss: 0.291141\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[87]\ttraining's binary_logloss: 0.28876\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\ttraining's binary_logloss: 0.286421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[89]\ttraining's binary_logloss: 0.284098\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[90]\ttraining's binary_logloss: 0.28171\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[91]\ttraining's binary_logloss: 0.279301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[92]\ttraining's binary_logloss: 0.276926\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[93]\ttraining's binary_logloss: 0.274585\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[94]\ttraining's binary_logloss: 0.272479\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[95]\ttraining's binary_logloss: 0.270383\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[96]\ttraining's binary_logloss: 0.2683\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[97]\ttraining's binary_logloss: 0.265824\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[98]\ttraining's binary_logloss: 0.263384\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\ttraining's binary_logloss: 0.260978\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttraining's binary_logloss: 0.258605\n",
      "Save model —— GBDT...\n",
      "Start predicting —— GBDT（generating feature vectors —— training data）...\n",
      "Writing transformed training data...\n",
      "Start predicting —— GBDT（generating feature vectors —— testing data）...\n",
      "Writing transformed testing data...\n",
      "Calculate feature importances...\n",
      "Feature importances: [41, 113, 19, 16, 24, 16, 24, 93, 10, 45, 17, 45, 30, 23, 6, 15, 8, 15, 60, 11, 26, 93, 143, 45, 82, 17, 32, 124, 41, 30]\n",
      "Feature importances: [54.25109807993948, 145.01525326994965, 18.778125095993346, 34.892693647415854, 50.49088830006622, 0.8860793924866925, 32.965811785102204, 5885.6398257160745, 0.018137268685677554, 6.83600288040634, 0.0009078391669845327, 4.7238086432266755, 4.125670794051985, 89.80762726842187, 0.5044541959697, 3.222557064486864, 0.5123110291002604, 2.032080861071411, 1.4251106888027607, 0.8578253626601509, 144.5845705440986, 218.7097156330144, 3062.86937097996, 295.1767450235043, 118.204144207923, 10.053998750241476, 58.655874572325594, 3204.204188877951, 4.797022549998928, 4.182402163413968]\n",
      "LogIstic Regression Start...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data is 398\n",
      "[[0.28473296 0.71526704]\n",
      " [0.98491514 0.01508486]\n",
      " [0.00992948 0.99007052]\n",
      " [0.01120593 0.98879407]\n",
      " [0.0075064  0.9924936 ]\n",
      " [0.87400335 0.12599665]\n",
      " [0.99674291 0.00325709]\n",
      " [0.03631257 0.96368743]\n",
      " [0.03797126 0.96202874]\n",
      " [0.9915158  0.0084842 ]\n",
      " [0.98631679 0.01368321]\n",
      " [0.98805838 0.01194162]\n",
      " [0.98797803 0.01202197]\n",
      " [0.00828239 0.99171761]\n",
      " [0.96577345 0.03422655]\n",
      " [0.98767451 0.01232549]\n",
      " [0.01182014 0.98817986]\n",
      " [0.98734546 0.01265454]\n",
      " [0.01012991 0.98987009]\n",
      " [0.78591331 0.21408669]\n",
      " [0.00901122 0.99098878]\n",
      " [0.01050726 0.98949274]\n",
      " [0.00784622 0.99215378]\n",
      " [0.80677437 0.19322563]\n",
      " [0.98033021 0.01966979]\n",
      " [0.01146535 0.98853465]\n",
      " [0.01234148 0.98765852]\n",
      " [0.98594741 0.01405259]\n",
      " [0.79018347 0.20981653]\n",
      " [0.04702357 0.95297643]\n",
      " [0.01037234 0.98962766]\n",
      " [0.03425593 0.96574407]\n",
      " [0.84940673 0.15059327]\n",
      " [0.00912036 0.99087964]\n",
      " [0.97714059 0.02285941]\n",
      " [0.98060316 0.01939684]\n",
      " [0.99274426 0.00725574]\n",
      " [0.05069658 0.94930342]\n",
      " [0.00684441 0.99315559]\n",
      " [0.08546453 0.91453547]\n",
      " [0.19108842 0.80891158]\n",
      " [0.01231603 0.98768397]\n",
      " [0.01230509 0.98769491]\n",
      " [0.02433152 0.97566848]\n",
      " [0.01263681 0.98736319]\n",
      " [0.98535214 0.01464786]\n",
      " [0.99026327 0.00973673]\n",
      " [0.01236231 0.98763769]\n",
      " [0.98437662 0.01562338]\n",
      " [0.98523253 0.01476747]\n",
      " [0.00844425 0.99155575]\n",
      " [0.00755376 0.99244624]\n",
      " [0.95891658 0.04108342]\n",
      " [0.01819951 0.98180049]\n",
      " [0.01333076 0.98666924]\n",
      " [0.00803344 0.99196656]\n",
      " [0.99595737 0.00404263]\n",
      " [0.01296417 0.98703583]\n",
      " [0.00991318 0.99008682]\n",
      " [0.00989101 0.99010899]\n",
      " [0.99794849 0.00205151]\n",
      " [0.98702217 0.01297783]\n",
      " [0.96487647 0.03512353]\n",
      " [0.00815803 0.99184197]\n",
      " [0.00784632 0.99215368]\n",
      " [0.9857847  0.0142153 ]\n",
      " [0.00827805 0.99172195]\n",
      " [0.98710394 0.01289606]\n",
      " [0.99035355 0.00964645]\n",
      " [0.04150868 0.95849132]\n",
      " [0.98353282 0.01646718]\n",
      " [0.02921905 0.97078095]\n",
      " [0.98165413 0.01834587]\n",
      " [0.033908   0.966092  ]\n",
      " [0.01426525 0.98573475]\n",
      " [0.00552043 0.99447957]\n",
      " [0.03186673 0.96813327]\n",
      " [0.006951   0.993049  ]\n",
      " [0.98151024 0.01848976]\n",
      " [0.01258954 0.98741046]\n",
      " [0.01674187 0.98325813]\n",
      " [0.98235734 0.01764266]\n",
      " [0.94946425 0.05053575]\n",
      " [0.9854653  0.0145347 ]\n",
      " [0.01617952 0.98382048]\n",
      " [0.16646377 0.83353623]\n",
      " [0.98669188 0.01330812]\n",
      " [0.00856908 0.99143092]\n",
      " [0.02823491 0.97176509]\n",
      " [0.01067824 0.98932176]\n",
      " [0.01003183 0.98996817]\n",
      " [0.9870129  0.0129871 ]\n",
      " [0.00868481 0.99131519]\n",
      " [0.00914453 0.99085547]\n",
      " [0.00977882 0.99022118]\n",
      " [0.86235271 0.13764729]\n",
      " [0.9891615  0.0108385 ]\n",
      " [0.01002582 0.98997418]\n",
      " [0.59380351 0.40619649]\n",
      " [0.9848364  0.0151636 ]\n",
      " [0.98228269 0.01771731]\n",
      " [0.01162402 0.98837598]\n",
      " [0.02107416 0.97892584]\n",
      " [0.01627161 0.98372839]\n",
      " [0.02075326 0.97924674]\n",
      " [0.01716172 0.98283828]\n",
      " [0.95857151 0.04142849]\n",
      " [0.01303197 0.98696803]\n",
      " [0.00776502 0.99223498]\n",
      " [0.01278062 0.98721938]\n",
      " [0.00938009 0.99061991]\n",
      " [0.09502052 0.90497948]\n",
      " [0.99769159 0.00230841]\n",
      " [0.97915424 0.02084576]\n",
      " [0.00889707 0.99110293]\n",
      " [0.2815137  0.7184863 ]\n",
      " [0.01125515 0.98874485]\n",
      " [0.98425727 0.01574273]\n",
      " [0.00785034 0.99214966]\n",
      " [0.99445262 0.00554738]\n",
      " [0.98751822 0.01248178]\n",
      " [0.95911112 0.04088888]\n",
      " [0.98532077 0.01467923]\n",
      " [0.15610341 0.84389659]\n",
      " [0.77127544 0.22872456]\n",
      " [0.98792791 0.01207209]\n",
      " [0.98343958 0.01656042]\n",
      " [0.01490244 0.98509756]\n",
      " [0.02233488 0.97766512]\n",
      " [0.01251161 0.98748839]\n",
      " [0.0081362  0.9918638 ]\n",
      " [0.98423551 0.01576449]\n",
      " [0.98265902 0.01734098]\n",
      " [0.01593675 0.98406325]\n",
      " [0.01149786 0.98850214]\n",
      " [0.00922954 0.99077046]\n",
      " [0.97952625 0.02047375]\n",
      " [0.99641796 0.00358204]\n",
      " [0.0107804  0.9892196 ]\n",
      " [0.00887538 0.99112462]\n",
      " [0.01061855 0.98938145]\n",
      " [0.98367533 0.01632467]\n",
      " [0.09487684 0.90512316]\n",
      " [0.01081206 0.98918794]\n",
      " [0.05643257 0.94356743]\n",
      " [0.01868138 0.98131862]\n",
      " [0.97533703 0.02466297]\n",
      " [0.00258058 0.99741942]\n",
      " [0.01323912 0.98676088]\n",
      " [0.69576695 0.30423305]\n",
      " [0.00951749 0.99048251]\n",
      " [0.00836699 0.99163301]\n",
      " [0.01392813 0.98607187]\n",
      " [0.01688995 0.98311005]\n",
      " [0.97392616 0.02607384]\n",
      " [0.98517123 0.01482877]\n",
      " [0.00994115 0.99005885]\n",
      " [0.99047599 0.00952401]\n",
      " [0.93472332 0.06527668]\n",
      " [0.02740484 0.97259516]\n",
      " [0.01074211 0.98925789]\n",
      " [0.00834804 0.99165196]\n",
      " [0.98806477 0.01193523]\n",
      " [0.00786313 0.99213687]\n",
      " [0.02454962 0.97545038]\n",
      " [0.00993952 0.99006048]\n",
      " [0.12496084 0.87503916]\n",
      " [0.27317532 0.72682468]\n",
      " [0.04702977 0.95297023]\n",
      " [0.15634152 0.84365848]\n",
      " [0.9435596  0.0564404 ]\n",
      " [0.01133503 0.98866497]\n",
      " [0.19630548 0.80369452]\n",
      " [0.98928033 0.01071967]\n",
      " [0.01030781 0.98969219]\n",
      " [0.98926576 0.01073424]\n",
      " [0.98754159 0.01245841]\n",
      " [0.00846873 0.99153127]\n",
      " [0.98340995 0.01659005]\n",
      " [0.01469004 0.98530996]\n",
      " [0.01038075 0.98961925]\n",
      " [0.98405969 0.01594031]\n",
      " [0.98503031 0.01496969]\n",
      " [0.03704907 0.96295093]\n",
      " [0.00969939 0.99030061]\n",
      " [0.01292765 0.98707235]\n",
      " [0.97951403 0.02048597]\n",
      " [0.02571109 0.97428891]\n",
      " [0.0248143  0.9751857 ]\n",
      " [0.00731437 0.99268563]\n",
      " [0.01247088 0.98752912]\n",
      " [0.0160587  0.9839413 ]\n",
      " [0.9888412  0.0111588 ]\n",
      " [0.98090714 0.01909286]\n",
      " [0.00721411 0.99278589]\n",
      " [0.99171606 0.00828394]\n",
      " [0.00896806 0.99103194]\n",
      " [0.98443456 0.01556544]\n",
      " [0.01607559 0.98392441]\n",
      " [0.01001953 0.98998047]\n",
      " [0.96405736 0.03594264]\n",
      " [0.88170775 0.11829225]\n",
      " [0.98740224 0.01259776]\n",
      " [0.98188105 0.01811895]\n",
      " [0.01861295 0.98138705]\n",
      " [0.98883369 0.01116631]\n",
      " [0.97127198 0.02872802]\n",
      " [0.01439607 0.98560393]\n",
      " [0.9942232  0.0057768 ]\n",
      " [0.01556906 0.98443094]\n",
      " [0.01012161 0.98987839]\n",
      " [0.00945346 0.99054654]\n",
      " [0.01205702 0.98794298]\n",
      " [0.79068647 0.20931353]\n",
      " [0.00487479 0.99512521]\n",
      " [0.0124584  0.9875416 ]\n",
      " [0.0189508  0.9810492 ]\n",
      " [0.98535808 0.01464192]\n",
      " [0.01015155 0.98984845]\n",
      " [0.01063727 0.98936273]\n",
      " [0.98411009 0.01588991]\n",
      " [0.98412171 0.01587829]\n",
      " [0.01039836 0.98960164]\n",
      " [0.98115508 0.01884492]\n",
      " [0.00800626 0.99199374]\n",
      " [0.00666006 0.99333994]\n",
      " [0.9821055  0.0178945 ]\n",
      " [0.05449227 0.94550773]\n",
      " [0.01040596 0.98959404]\n",
      " [0.00776792 0.99223208]\n",
      " [0.00760429 0.99239571]\n",
      " [0.98325107 0.01674893]\n",
      " [0.00754596 0.99245404]\n",
      " [0.01648166 0.98351834]\n",
      " [0.98450431 0.01549569]\n",
      " [0.01000194 0.98999806]\n",
      " [0.00762099 0.99237901]\n",
      " [0.0099482  0.9900518 ]\n",
      " [0.00766052 0.99233948]\n",
      " [0.00973578 0.99026422]\n",
      " [0.00904478 0.99095522]\n",
      " [0.02236754 0.97763246]\n",
      " [0.01013733 0.98986267]\n",
      " [0.010004   0.989996  ]\n",
      " [0.10159731 0.89840269]\n",
      " [0.97860602 0.02139398]\n",
      " [0.9824658  0.0175342 ]\n",
      " [0.97790051 0.02209949]\n",
      " [0.01166804 0.98833196]\n",
      " [0.00860024 0.99139976]\n",
      " [0.00780549 0.99219451]\n",
      " [0.14836142 0.85163858]\n",
      " [0.01483346 0.98516654]\n",
      " [0.98307243 0.01692757]\n",
      " [0.01100426 0.98899574]\n",
      " [0.9874429  0.0125571 ]\n",
      " [0.006348   0.993652  ]\n",
      " [0.08784231 0.91215769]\n",
      " [0.01635726 0.98364274]\n",
      " [0.01519821 0.98480179]\n",
      " [0.0099624  0.9900376 ]\n",
      " [0.01140039 0.98859961]\n",
      " [0.01493183 0.98506817]\n",
      " [0.98252626 0.01747374]\n",
      " [0.05026073 0.94973927]\n",
      " [0.74313834 0.25686166]\n",
      " [0.9488289  0.0511711 ]\n",
      " [0.02845627 0.97154373]\n",
      " [0.00316637 0.99683363]\n",
      " [0.01365541 0.98634459]\n",
      " [0.00958288 0.99041712]\n",
      " [0.00794251 0.99205749]\n",
      " [0.00703919 0.99296081]\n",
      " [0.98422996 0.01577004]\n",
      " [0.98635809 0.01364191]\n",
      " [0.97988419 0.02011581]\n",
      " [0.00886833 0.99113167]\n",
      " [0.01298044 0.98701956]\n",
      " [0.0095484  0.9904516 ]\n",
      " [0.00612075 0.99387925]\n",
      " [0.98502026 0.01497974]\n",
      " [0.96319465 0.03680535]\n",
      " [0.01451909 0.98548091]\n",
      " [0.98635905 0.01364095]\n",
      " [0.98563984 0.01436016]\n",
      " [0.01317117 0.98682883]\n",
      " [0.00869271 0.99130729]\n",
      " [0.0102016  0.9897984 ]\n",
      " [0.00807696 0.99192304]\n",
      " [0.00738428 0.99261572]\n",
      " [0.00998065 0.99001935]\n",
      " [0.8596803  0.1403197 ]\n",
      " [0.99272293 0.00727707]\n",
      " [0.9807015  0.0192985 ]\n",
      " [0.99101294 0.00898706]\n",
      " [0.02035217 0.97964783]\n",
      " [0.02136647 0.97863353]\n",
      " [0.9871225  0.0128775 ]\n",
      " [0.00985186 0.99014814]\n",
      " [0.00865361 0.99134639]\n",
      " [0.00986071 0.99013929]\n",
      " [0.00830353 0.99169647]\n",
      " [0.01164624 0.98835376]\n",
      " [0.01003875 0.98996125]\n",
      " [0.02698241 0.97301759]\n",
      " [0.00898114 0.99101886]\n",
      " [0.01587657 0.98412343]\n",
      " [0.00938461 0.99061539]\n",
      " [0.0129421  0.9870579 ]\n",
      " [0.89719547 0.10280453]\n",
      " [0.01152125 0.98847875]\n",
      " [0.98933543 0.01066457]\n",
      " [0.94104104 0.05895896]\n",
      " [0.01002668 0.98997332]\n",
      " [0.01033446 0.98966554]\n",
      " [0.00922144 0.99077856]\n",
      " [0.95292673 0.04707327]\n",
      " [0.90452355 0.09547645]\n",
      " [0.01353859 0.98646141]\n",
      " [0.03399891 0.96600109]\n",
      " [0.0144038  0.9855962 ]\n",
      " [0.98079793 0.01920207]\n",
      " [0.98969041 0.01030959]\n",
      " [0.9837236  0.0162764 ]\n",
      " [0.99060568 0.00939432]\n",
      " [0.02012776 0.97987224]\n",
      " [0.01508596 0.98491404]\n",
      " [0.99492062 0.00507938]\n",
      " [0.01078651 0.98921349]\n",
      " [0.97932954 0.02067046]\n",
      " [0.00525826 0.99474174]\n",
      " [0.98771443 0.01228557]\n",
      " [0.00922247 0.99077753]\n",
      " [0.01963839 0.98036161]\n",
      " [0.00951099 0.99048901]\n",
      " [0.01728774 0.98271226]\n",
      " [0.98854684 0.01145316]\n",
      " [0.9791679  0.0208321 ]\n",
      " [0.25700617 0.74299383]\n",
      " [0.01041428 0.98958572]\n",
      " [0.91678664 0.08321336]\n",
      " [0.01118329 0.98881671]\n",
      " [0.87888592 0.12111408]\n",
      " [0.98792792 0.01207208]\n",
      " [0.00865765 0.99134235]\n",
      " [0.94071641 0.05928359]\n",
      " [0.95214312 0.04785688]\n",
      " [0.98435087 0.01564913]\n",
      " [0.01064368 0.98935632]\n",
      " [0.99390227 0.00609773]\n",
      " [0.01217557 0.98782443]\n",
      " [0.94305137 0.05694863]\n",
      " [0.01083081 0.98916919]\n",
      " [0.98973341 0.01026659]\n",
      " [0.01137188 0.98862812]\n",
      " [0.01351608 0.98648392]\n",
      " [0.98680945 0.01319055]\n",
      " [0.01233512 0.98766488]\n",
      " [0.06116633 0.93883367]\n",
      " [0.01080083 0.98919917]\n",
      " [0.67150507 0.32849493]\n",
      " [0.96787513 0.03212487]\n",
      " [0.29631891 0.70368109]\n",
      " [0.98482035 0.01517965]\n",
      " [0.98143665 0.01856335]\n",
      " [0.01453275 0.98546725]\n",
      " [0.01203925 0.98796075]\n",
      " [0.98302403 0.01697597]\n",
      " [0.96078602 0.03921398]\n",
      " [0.01428358 0.98571642]\n",
      " [0.00878097 0.99121903]\n",
      " [0.98815442 0.01184558]\n",
      " [0.99203166 0.00796834]\n",
      " [0.01359348 0.98640652]\n",
      " [0.01036307 0.98963693]\n",
      " [0.03574598 0.96425402]\n",
      " [0.00805572 0.99194428]\n",
      " [0.01113336 0.98886664]\n",
      " [0.96893516 0.03106484]\n",
      " [0.06230879 0.93769121]\n",
      " [0.00967881 0.99032119]\n",
      " [0.05405075 0.94594925]\n",
      " [0.01500223 0.98499777]\n",
      " [0.96966859 0.03033141]\n",
      " [0.00872688 0.99127312]\n",
      " [0.99249533 0.00750467]\n",
      " [0.97865401 0.02134599]\n",
      " [0.98409246 0.01590754]\n",
      " [0.98516713 0.01483287]\n",
      " [0.01138791 0.98861209]\n",
      " [0.99263876 0.00736124]\n",
      " [0.00776752 0.99223248]\n",
      " [0.71657229 0.28342771]\n",
      " [0.01127717 0.98872283]\n",
      " [0.00807641 0.99192359]\n",
      " [0.9848096  0.0151904 ]\n",
      " [0.98487356 0.01512644]\n",
      " [0.01139966 0.98860034]]\n",
      "number of testing data is 171\n",
      "[[0.01021067 0.98978933]\n",
      " [0.01012665 0.98987335]\n",
      " [0.00976036 0.99023964]\n",
      " [0.9454799  0.0545201 ]\n",
      " [0.01001008 0.98998992]\n",
      " [0.20393975 0.79606025]\n",
      " [0.98927158 0.01072842]\n",
      " [0.06917021 0.93082979]\n",
      " [0.00994387 0.99005613]\n",
      " [0.98244852 0.01755148]\n",
      " [0.00607731 0.99392269]\n",
      " [0.06590839 0.93409161]\n",
      " [0.0844099  0.9155901 ]\n",
      " [0.01193811 0.98806189]\n",
      " [0.00766131 0.99233869]\n",
      " [0.98424344 0.01575656]\n",
      " [0.99445643 0.00554357]\n",
      " [0.04474406 0.95525594]\n",
      " [0.01435821 0.98564179]\n",
      " [0.01612108 0.98387892]\n",
      " [0.00978931 0.99021069]\n",
      " [0.14427076 0.85572924]\n",
      " [0.00711385 0.99288615]\n",
      " [0.00842487 0.99157513]\n",
      " [0.0095317  0.9904683 ]\n",
      " [0.98834869 0.01165131]\n",
      " [0.98563776 0.01436224]\n",
      " [0.99423345 0.00576655]\n",
      " [0.00862014 0.99137986]\n",
      " [0.98819793 0.01180207]\n",
      " [0.01847848 0.98152152]\n",
      " [0.01011139 0.98988861]\n",
      " [0.01539919 0.98460081]\n",
      " [0.0238725  0.9761275 ]\n",
      " [0.42270851 0.57729149]\n",
      " [0.01175795 0.98824205]\n",
      " [0.95018173 0.04981827]\n",
      " [0.00876807 0.99123193]\n",
      " [0.29426754 0.70573246]\n",
      " [0.00738002 0.99261998]\n",
      " [0.00973043 0.99026957]\n",
      " [0.01742156 0.98257844]\n",
      " [0.30409884 0.69590116]\n",
      " [0.01769296 0.98230704]\n",
      " [0.00823688 0.99176312]\n",
      " [0.97885539 0.02114461]\n",
      " [0.0331375  0.9668625 ]\n",
      " [0.00760548 0.99239452]\n",
      " [0.0317726  0.9682274 ]\n",
      " [0.01097879 0.98902121]\n",
      " [0.98720972 0.01279028]\n",
      " [0.00646309 0.99353691]\n",
      " [0.01290952 0.98709048]\n",
      " [0.01123205 0.98876795]\n",
      " [0.01428113 0.98571887]\n",
      " [0.00966843 0.99033157]\n",
      " [0.29450643 0.70549357]\n",
      " [0.03069522 0.96930478]\n",
      " [0.38307739 0.61692261]\n",
      " [0.98944016 0.01055984]\n",
      " [0.98827891 0.01172109]\n",
      " [0.01009568 0.98990432]\n",
      " [0.97968614 0.02031386]\n",
      " [0.01015284 0.98984716]\n",
      " [0.01353053 0.98646947]\n",
      " [0.02527038 0.97472962]\n",
      " [0.982982   0.017018  ]\n",
      " [0.97232088 0.02767912]\n",
      " [0.02295064 0.97704936]\n",
      " [0.02997154 0.97002846]\n",
      " [0.02085744 0.97914256]\n",
      " [0.01060226 0.98939774]\n",
      " [0.02367613 0.97632387]\n",
      " [0.01457621 0.98542379]\n",
      " [0.02892489 0.97107511]\n",
      " [0.98643761 0.01356239]\n",
      " [0.00985798 0.99014202]\n",
      " [0.98210182 0.01789818]\n",
      " [0.00788321 0.99211679]\n",
      " [0.00874002 0.99125998]\n",
      " [0.01053083 0.98946917]\n",
      " [0.03372384 0.96627616]\n",
      " [0.02684749 0.97315251]\n",
      " [0.01281734 0.98718266]\n",
      " [0.98967527 0.01032473]\n",
      " [0.00962407 0.99037593]\n",
      " [0.01392292 0.98607708]\n",
      " [0.01242455 0.98757545]\n",
      " [0.99154344 0.00845656]\n",
      " [0.00804324 0.99195676]\n",
      " [0.0088666  0.9911334 ]\n",
      " [0.98852001 0.01147999]\n",
      " [0.98664464 0.01335536]\n",
      " [0.01826403 0.98173597]\n",
      " [0.0085807  0.9914193 ]\n",
      " [0.01074224 0.98925776]\n",
      " [0.00954163 0.99045837]\n",
      " [0.98838752 0.01161248]\n",
      " [0.02621038 0.97378962]\n",
      " [0.01861106 0.98138894]\n",
      " [0.14548483 0.85451517]\n",
      " [0.008772   0.991228  ]\n",
      " [0.00876813 0.99123187]\n",
      " [0.02192274 0.97807726]\n",
      " [0.98298782 0.01701218]\n",
      " [0.98290467 0.01709533]\n",
      " [0.98065279 0.01934721]\n",
      " [0.30717592 0.69282408]\n",
      " [0.93024374 0.06975626]\n",
      " [0.01125541 0.98874459]\n",
      " [0.03012455 0.96987545]\n",
      " [0.00653714 0.99346286]\n",
      " [0.98976193 0.01023807]\n",
      " [0.21980733 0.78019267]\n",
      " [0.05226147 0.94773853]\n",
      " [0.33404778 0.66595222]\n",
      " [0.00958991 0.99041009]\n",
      " [0.01010853 0.98989147]\n",
      " [0.03322177 0.96677823]\n",
      " [0.02507123 0.97492877]\n",
      " [0.0078839  0.9921161 ]\n",
      " [0.09994279 0.90005721]\n",
      " [0.00852862 0.99147138]\n",
      " [0.92657931 0.07342069]\n",
      " [0.99596611 0.00403389]\n",
      " [0.0068296  0.9931704 ]\n",
      " [0.01005155 0.98994845]\n",
      " [0.98616087 0.01383913]\n",
      " [0.00947398 0.99052602]\n",
      " [0.03397311 0.96602689]\n",
      " [0.99015921 0.00984079]\n",
      " [0.01976004 0.98023996]\n",
      " [0.99071167 0.00928833]\n",
      " [0.68359063 0.31640937]\n",
      " [0.01069564 0.98930436]\n",
      " [0.01224259 0.98775741]\n",
      " [0.00826407 0.99173593]\n",
      " [0.97625929 0.02374071]\n",
      " [0.9859472  0.0140528 ]\n",
      " [0.88435505 0.11564495]\n",
      " [0.00839755 0.99160245]\n",
      " [0.99253496 0.00746504]\n",
      " [0.19321678 0.80678322]\n",
      " [0.9824013  0.0175987 ]\n",
      " [0.98698368 0.01301632]\n",
      " [0.01244967 0.98755033]\n",
      " [0.47024793 0.52975207]\n",
      " [0.02270196 0.97729804]\n",
      " [0.01639696 0.98360304]\n",
      " [0.01307925 0.98692075]\n",
      " [0.00738769 0.99261231]\n",
      " [0.00954468 0.99045532]\n",
      " [0.98320343 0.01679657]\n",
      " [0.98806521 0.01193479]\n",
      " [0.98944213 0.01055787]\n",
      " [0.54564487 0.45435513]\n",
      " [0.00789366 0.99210634]\n",
      " [0.00738929 0.99261071]\n",
      " [0.98398781 0.01601219]\n",
      " [0.02839512 0.97160488]\n",
      " [0.982594   0.017406  ]\n",
      " [0.89008683 0.10991317]\n",
      " [0.01248974 0.98751026]\n",
      " [0.16021992 0.83978008]\n",
      " [0.98872742 0.01127258]\n",
      " [0.06572691 0.93427309]\n",
      " [0.00766562 0.99233438]\n",
      " [0.01200819 0.98799181]\n",
      " [0.01175496 0.98824504]\n",
      " [0.00579538 0.99420462]\n",
      " [0.01804068 0.98195932]]\n",
      "Start evaluating —— GBDT+LR...\n",
      "prediction accuracy of training data is 1.0\n",
      "Normalized Cross Entropy of training data is 0.7831814315501352\n",
      "prediction accuracy of testing data is 0.9473684210526315\n",
      "Normalized Cross Entropy of testing data is 0.6743588957304056\n"
     ]
    }
   ],
   "source": [
    "# 链接：[XGBoost和LightGBM的参数以及调参](https://www.jianshu.com/p/1100e333fcab)\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer  # breast cancer wisconsin dataset(classification)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# ===========================================================================================\n",
    "# 加载数据集\n",
    "# ===========================================================================================\n",
    "print('Load data...')\n",
    "\n",
    "df = load_breast_cancer()\n",
    "X = df.data  # (569, 30)\n",
    "y = df.target  # (569,)\n",
    "\n",
    "# ===========================================================================================\n",
    "# 划分训练集和测试集\n",
    "# ===========================================================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.data, df.target, test_size=0.3)\n",
    "\n",
    "\n",
    "'''***************************************** GBDT ****************************************'''\n",
    "# ===========================================================================================\n",
    "# 转换为 Dataset 数据格式\n",
    "# ===========================================================================================\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# ===========================================================================================\n",
    "# 参数 —— GBDT\n",
    "# ===========================================================================================\n",
    "params = {\n",
    "    'task': 'train', \n",
    "    'boosting_type': 'gbdt', \n",
    "    'objective': 'binary', \n",
    "    'metric': {'binary_logloss'}, \n",
    "    'num_leaves': 63, \n",
    "    'num_trees': 100, \n",
    "    'learning_rate': 0.01, \n",
    "    'feature_fraction': 0.9, \n",
    "    'bagging_fraction': 0.8, \n",
    "    'bagging_freq': 5, \n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# ===========================================================================================\n",
    "# 模型训练 —— GBDT\n",
    "# ===========================================================================================\n",
    "print('Start training —— GBDT...')\n",
    "\n",
    "# train\n",
    "gbm = lgb.train(params, lgb_train, num_boost_round=100, valid_sets=lgb_train)\n",
    "\n",
    "# ===========================================================================================\n",
    "# 模型保存 —— GBDT\n",
    "# ===========================================================================================\n",
    "print('Save model —— GBDT...')\n",
    "\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "# ===========================================================================================\n",
    "# 模型预测 —— GBDT（生成特征向量 —— 训练集/测试集）\n",
    "# ===========================================================================================\n",
    "# number of leaves, will be used in feature transformation\n",
    "num_leaf = 63\n",
    "\n",
    "print('Start predicting —— GBDT（generating feature vectors —— training data）...')\n",
    "\n",
    "# predict with leaf index of all trees\n",
    "y_train_pred = gbm.predict(X_train,pred_leaf=True)  # (398, 100)\n",
    "\n",
    "# feature transformation and write result\n",
    "print('Writing transformed training data...')\n",
    "transformed_training_matrix = np.zeros([len(y_train_pred),len(y_train_pred[0]) * num_leaf], \n",
    "                                       dtype=np.int64)  # (398, 6300)\n",
    "\n",
    "for i in range(0,len(y_train_pred)):\n",
    "    temp_train = np.arange(len(y_train_pred[0])) * num_leaf - 1 + np.array(y_train_pred[i])\n",
    "    transformed_training_matrix[i][temp_train] += 1\n",
    "\n",
    "# for i in range(0,len(y_train_pred)):\n",
    "#     for j in range(0,len(y_train_pred[i])):\n",
    "#         transformed_training_matrix[i][j * num_leaf + y_train_pred[i][j]-1] = 1\n",
    "\n",
    "\n",
    "\n",
    "print('Start predicting —— GBDT（generating feature vectors —— testing data）...')\n",
    "\n",
    "# predict with leaf index of all trees\n",
    "y_test_pred = gbm.predict(X_test,pred_leaf=True)\n",
    "\n",
    "# feature transformation and write result\n",
    "print('Writing transformed testing data...')\n",
    "transformed_testing_matrix = np.zeros([len(y_test_pred),len(y_test_pred[0]) * num_leaf], \n",
    "                                      dtype=np.int64)\n",
    "for i in range(0,len(y_test_pred)):\n",
    "    temp_test = np.arange(len(y_test_pred[0])) * num_leaf - 1 + np.array(y_test_pred[i])\n",
    "    transformed_testing_matrix[i][temp_test] += 1\n",
    "\n",
    "# for i in range(0,len(y_test_pred)):\n",
    "#     for j in range(0,len(y_test_pred[i])):\n",
    "#         transformed_testing_matrix[i][j * num_leaf + y_test_pred[i][j]-1] = 1\n",
    "\n",
    "# ===========================================================================================\n",
    "# 特征重要性\n",
    "# ===========================================================================================\n",
    "print('Calculate feature importances...')\n",
    "\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importance()))\n",
    "print('Feature importances:', list(gbm.feature_importance(\"gain\")))\n",
    "\n",
    "\n",
    "'''********************************** Logistic Regression ********************************'''\n",
    "# ===========================================================================================\n",
    "# 逻辑回归\n",
    "# ===========================================================================================\n",
    "print(\"LogIstic Regression Start...\")\n",
    "\n",
    "lm = LogisticRegression(penalty='l2',C=0.1) # logestic model construction\n",
    "lm.fit(transformed_training_matrix,y_train)  # fitting the data\n",
    "\n",
    "# training data\n",
    "y_pred_label_train = lm.predict(transformed_training_matrix )\n",
    "y_pred_est_train = lm.predict_proba(transformed_training_matrix)   # Give the probabilty on each label\n",
    "print('number of training data is ' + str(len(y_pred_label_train)))\n",
    "print(y_pred_est_train)\n",
    "\n",
    "# testing data\n",
    "y_pred_label_test = lm.predict(transformed_testing_matrix)\n",
    "y_pred_est_test = lm.predict_proba(transformed_testing_matrix)   # Give the probabilty on each label\n",
    "print('number of testing data is ' + str(len(y_pred_label_test)))\n",
    "print(y_pred_est_test)\n",
    "\n",
    "# ===========================================================================================\n",
    "# 模型评估\n",
    "# ===========================================================================================\n",
    "print(\"Start evaluating —— GBDT+LR...\")\n",
    "\n",
    "# calculate predict accuracy —— training data\n",
    "num_train = 0\n",
    "for i in range(0,len(y_pred_label_train)):\n",
    "    if y_train[i] == y_pred_label_train[i]:\n",
    "        num_train += 1\n",
    "print(\"prediction accuracy of training data is \" + str((num_train)/len(y_pred_label_train)))\n",
    "\n",
    "# calculate the Normalized Cross-Entropy —— training data\n",
    "NE_train = (-1) / len(y_pred_est_train) * sum(((1+y_train)/2 * np.log(y_pred_est_train[:,1]) +  \n",
    "                                               (1-y_train)/2 * np.log(1 - y_pred_est_train[:,1])))\n",
    "print(\"Normalized Cross Entropy of training data is \" + str(NE_train))\n",
    "\n",
    "\n",
    "\n",
    "# calculate predict accuracy —— testing data\n",
    "num_test = 0\n",
    "for i in range(0,len(y_pred_label_test)):\n",
    "    if y_test[i] == y_pred_label_test[i]:\n",
    "        num_test += 1\n",
    "print(\"prediction accuracy of testing data is \" + str((num_test)/len(y_pred_label_test)))\n",
    "    \n",
    "# calculate the Normalized Cross-Entropy —— testing data\n",
    "NE_test = (-1) / len(y_pred_est_test) * sum(((1+y_test)/2 * np.log(y_pred_est_test[:,1]) +  \n",
    "                                               (1-y_test)/2 * np.log(1 - y_pred_est_test[:,1])))\n",
    "print(\"Normalized Cross Entropy of testing data is \" + str(NE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
